{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [Errno -2]\n",
      "[nltk_data]     Name or service not known>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation work\n",
    "def create_db(name):\n",
    "    headers = {'content-type': 'application/json'}\n",
    "    url = 'http://admin:admin@172.26.134.87:5984/' + name\n",
    "    r = requests.put(url)\n",
    "    print(r.text)\n",
    "\n",
    "# create_db('twitter')\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'W3nWSuPyudnw8142u58LNXiTc'\n",
    "consumer_secret = 'cNTNL1tBB9lQKNaIr11u1CLv0IMBRzc81JS7QqRLCNXy6b334p'\n",
    "access_token = '3149835139-Ey1XqWLn6Mk1MFKcHbTtaDJ9NZUETZJYgISfLjW'\n",
    "token_secret = 'Y0ZffGkSBbaYYSkvitsEunU9gyj2EAERWMxUhAaiPe30k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    cleaned = ' '.join(re.sub(\"(@[A-Za-z0-9_-]+)|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return cleaned\n",
    "\n",
    "def getSentimentScores(tweet,sid=analyzer):\n",
    "    ss = sid.polarity_scores(tweet)['compound']\n",
    "    return ss\n",
    "\n",
    "def bulk_upload(payload):\n",
    "    headers = {'content-type': 'application/json'}\n",
    "    url = 'http://admin:admin@172.26.134.87:5984/twitter/_bulk_docs'\n",
    "    r = requests.post(url, data=payload, headers=headers)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394882398907625474\n",
      "Wed May 19 05:07:34 +0000 2021\n",
      "1394876743102738432\n",
      "Wed May 19 04:45:05 +0000 2021\n",
      "1394857203425845250\n",
      "Wed May 19 03:27:27 +0000 2021\n",
      "1394856585252544515\n",
      "Wed May 19 03:24:59 +0000 2021\n",
      "1394853822850289665\n",
      "Wed May 19 03:14:01 +0000 2021\n",
      "1394850893351247878\n",
      "Wed May 19 03:02:22 +0000 2021\n",
      "1394850605089308675\n",
      "Wed May 19 03:01:13 +0000 2021\n",
      "1394850560973721609\n",
      "Wed May 19 03:01:03 +0000 2021\n",
      "1394601437951188993\n",
      "Tue May 18 10:31:07 +0000 2021\n",
      "1394577263568457728\n",
      "Tue May 18 08:55:04 +0000 2021\n"
     ]
    }
   ],
   "source": [
    "def harvest1(city, n, max_id):\n",
    "#     print('max_id', max_id)\n",
    "    tid = []\n",
    "    created_at = []\n",
    "    tweet_text = []\n",
    "    retweet_counts = []\n",
    "    favorite_count = []\n",
    "    hashtags = []\n",
    "    tweet_ss = []\n",
    "    \n",
    "#     , 'house market {}'.format(city), 'real estate market {}'.format(city)\n",
    "    queries = ['buy house {}'.format(city)]\n",
    "    for query in queries:\n",
    "#         print('\\n')\n",
    "#         print(query)\n",
    "        cursor = tweepy.Cursor(api.search,q=query,tweet_mode=\"extended\").items(n)\n",
    "        for i in cursor:\n",
    "            tweet_info = i._json\n",
    "#             print(tweet_info)\n",
    "            try:\n",
    "                tweet_info['retweeted_status']\n",
    "            # only harvest non-retweeted tweets\n",
    "            except KeyError:\n",
    "                text = clean_tweet(tweet_info['full_text'])\n",
    "                ss = getSentimentScores(text)\n",
    "                \n",
    "                tid.append(tweet_info['id_str'])\n",
    "                created_at.append(tweet_info['created_at'])\n",
    "                tweet_text.append(text)\n",
    "                retweet_counts.append(tweet_info['retweet_count'])\n",
    "                favorite_count.append(tweet_info['favorite_count'])\n",
    "                hashtags.append(tweet_info['entities']['hashtags'])\n",
    "                tweet_ss.append(ss)\n",
    "                \n",
    "                print(tweet_info['id_str'])\n",
    "                print(tweet_info['created_at'])\n",
    "    return tid[-1], (tid, created_at, tweet_text, coordinates, retweet_counts, favorite_count, hashtags, tweet_ss)\n",
    "\n",
    "def prepare_data(tid, created_at, tweet_text, retweet_counts, favorite_count, hashtags, tweet_ss, city):\n",
    "    batch = []\n",
    "    zipped = zip(tid, created_at, tweet_text, retweet_counts, favorite_count, hashtags, tweet_ss)\n",
    "    for item in zipped:\n",
    "        tweet = {}\n",
    "        tweet['_id'] = item[0]\n",
    "        tweet['city'] = city\n",
    "        tweet['created_at'] = item[1]\n",
    "        tweet['text'] = item[2]\n",
    "        tweet['retweet_count'] = item[3]\n",
    "        tweet['favorite_count'] = item[4]\n",
    "        tweet['hashtag'] = item[5]\n",
    "        tweet['sentiment'] = item[6]\n",
    "        \n",
    "        batch.append(tweet)\n",
    "    docs = {'docs': batch}\n",
    "    docs = json.dumps(docs)\n",
    "    return docs\n",
    "\n",
    "count = 0 \n",
    "max_id = None\n",
    "# while count < 2:\n",
    "max_id, (tid, created_at, tweet_text, coordinates, retweet_counts, favorite_count, hashtags, tweet_ss) = harvest1('melbourne',20, max_id)\n",
    "#     docs = prepare_data(tid, created_at, tweet_text, coordinates, retweet_counts, favorite_count, hashtags, tweet_ss, 'melbourne')\n",
    "#     bulk_upload(docs)\n",
    "#     count += 1\n",
    "# time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
